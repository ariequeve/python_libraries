{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03 - Ejercicio Cross-validation y optimización de hiperparámetros.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNway4d2wYFwAJO77Z0UAVP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Ejercicio ##\n","1) Levantar la base finalOrders.txt creada en el ejercicio anterior.  \n","2) Dividir la base levantada en un 80% y 20%. El 20% lo dejaremos sin usar hasta que querramos validar todos los modelos creados.  \n","3) Crear un modelo lightgbm con los parámetros por defecto pero ejecutarlo con un cross-validation de 5 folds. ¿Qué AUC se obtiene del modelo creado anteriormente?. ¿Existe diferencias contra el lightGBM creado en el ejercicio anterior?  \n","4) En este caso, ¿qué es más recomendable, usar kfolds o StratifiedKfolds? ¿Por qué?  \n","5) Optimizar los hiperparámetros Con Random Search usando 15 iteraciones del modelo creado anteriormente. Los hiperparámetros a optimizar serán:\n","*   max_depth: 5,10,15,20\n","*   learning_rate: 0.1, 0.01, 0.001\n","*   n_estimators: 100, 200, 500, 1000\n","*   n_estimators: 100, 200, 500, 1000  \n","\n","Utilizar AUC como métrica de medición.\n","\n","6) ¿Cuáles fueron los hiperparámetros que mejor resultado dieron?  \n","7) ¿Cuál fue el AUC obtenido?  \n","8) Si utilizamos la optimización bayesiana, con 15 iteraciones. ¿Cuál es el conjunto de hiperparámetros con mejor resultado?  \n","9) ¿Qué AUC obtuvieron?  \n","10) Ejecutar un Grid Search con el espacio de hiperparámetros completos. ¿Cuántas ejecuciones se realizaron?  \n","11) ¿Cuáles fueron los hiperparámetros con mejor resultado? Qué diferencia hubo con el ejercicio 8)?  \n","12) Con el modelo obtenido en el ejercicio 11, obtener las probabilidades del 20% de la base que separamos en el ejercicio 2 y graficar la Curva ROC.\n","\n","\n"],"metadata":{"id":"sJV58DSTXcyU"}}]}